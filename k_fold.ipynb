{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6f21a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2.2. k-fold cross validation \n",
    "# using a training dataset from the gpr_eos model\n",
    "# (https://medium.com/datadriveninvestor/k-fold-cross-validation-6b8518070833)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4643dbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import model_selection\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79c0019d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for (random) kfold cross validation\n",
    "if __name__ == \"__main__\": # https://stackoverflow.com/questions/419163/what-does-if-name-main-do\n",
    "    # training data is in a CSV (train.csv - i have used my own here)\n",
    "    df = pd.read_csv(\"PTX_liq_All_reduced.csv\")\n",
    "    \n",
    "    # create a new column called kfold and fill with -1\n",
    "    df[\"kfold\"] = -1\n",
    "    \n",
    "    # randomize the rows of data\n",
    "    df = df.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    # initiate the kfold class from model_selection module\n",
    "    kf = model_selection.KFold(n_splits=5)\n",
    "    \n",
    "    # fill the new kfold column\n",
    "    for fold, (trn_, val_) in enumerate(kf.split(X=df)):\n",
    "        df.loc[val_, 'kfold'] = fold\n",
    "    \n",
    "    # save the new csv with kfold column\n",
    "    df.to_csv(\"train_folds.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "830545c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for stratified kfold cross validation\n",
    "# this type of kfold is useful when you have a skewed data set (e.g. 90% positive 10% negative)\n",
    "# in this case you don't want to use a random kfold, instead use stratified kfold where the ratio of labels is kept constant\n",
    "# I have made a random dataset to use here\n",
    "if __name__ == \"__main__\":\n",
    "    # training data is in a CSV\n",
    "    df = pd.read_csv(\"train_skew.csv\")\n",
    "    \n",
    "    # create a new column called kfold and fill with -1\n",
    "    df[\"kfold\"] = -1\n",
    "    \n",
    "    # randomize the rows of data\n",
    "    df = df.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    #fetch targets\n",
    "    y = df.target.values\n",
    "    \n",
    "    # initiate the kfold class from model_selection module\n",
    "    kf = model_selection.StratifiedKFold(n_splits=5)\n",
    "    \n",
    "    # fill the new kfold column\n",
    "    for f, (t_, v_) in enumerate(kf.split(X=df, y=y)):\n",
    "        df.loc[v_, 'kfold'] = f\n",
    "    \n",
    "    # save the new csv with kfold column\n",
    "    df.to_csv(\"train_folds_strat.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "74a94dc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'count')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# for the red wine quality dataset used before, lets look at the distribution of labels\n",
    "import seaborn as sns\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/insaid2018/Term-1/master/Data/Projects/winequality.csv')\n",
    "quality_mapping = {\n",
    "    3: 0,\n",
    "    4: 1,\n",
    "    5: 2,\n",
    "    6: 3,\n",
    "    7: 4,\n",
    "    8: 5\n",
    "}\n",
    "\n",
    "df.loc[:, \"quality\"] = df.quality.map(quality_mapping)\n",
    "\n",
    "b = sns.countplot(x='quality', data=df)\n",
    "b.set_xlabel(\"quality\", fontsize=20)\n",
    "b.set_ylabel(\"count\", fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4f0d1141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here it is clear that some labels have far more samples in that others\n",
    "# for a standard classification problem it is nearly always best to use stratified kfold\n",
    "\n",
    "## hold-out\n",
    "\n",
    "# however using a 5 fold cross validation on a dataset of 1 million samples would mean training on 800k samples\n",
    "# and testing on 200k - this can be very expensive and hold-out based validation should be used\n",
    "# the process for creating a hold-out dataset is the same as stratified k-fold\n",
    "# for 1 million samples with 10 folds, we take 900k samples for training and then always use the remaining 100k\n",
    "# samples to calculate accuracy metrics on\n",
    "\n",
    "# hold-out is frequently used with time-series data\n",
    "# say we have time steps 0 to 30 and we want to predicit 31 to 40, we can hold steps 21 to 30 for validation and train\n",
    "# on steps 0 to 20\n",
    "# NOTE: when using the model to predicit 31 to 40, steps 21 to 30 should be included in the model otherwise performance \n",
    "# will be sub-par\n",
    "\n",
    "## small datasets\n",
    "\n",
    "# for small datasets we can't afford to take out too much data, therefore use a type of kfold where k=N\n",
    "# here the number of folds (k) is equal to the number of samples (N)\n",
    "# this means in all the folds of training we are training on all the data, except 1 sample\n",
    "# this can be costly in terms of time, but as it is only used with small datasets this isn't too much of a problem\n",
    "\n",
    "## regression\n",
    "\n",
    "# now we can consider the regression case\n",
    "# can can use all the cross validation methods mentioned, except stratified kfold\n",
    "# generally for a regression problem a simple kfold will work, if however a distribution of target is not consistent\n",
    "# then it is possible to adapt the problem so that stratified kfold can be used for regression\n",
    "\n",
    "# to perform a stratified kfold in a regression setting we first need to divide the target into bins\n",
    "# for large (> 10k, > 100k) datasets, it doesn't really matter how many bins are used, 10 or 20 are fine\n",
    "# for smaller datasets Sturge's Rule should be used:\n",
    "\n",
    "# Number of Bins = 1 + log2(N)\n",
    "\n",
    "# where N is the number of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "84765c29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f_0</th>\n",
       "      <th>f_1</th>\n",
       "      <th>f_2</th>\n",
       "      <th>f_3</th>\n",
       "      <th>f_4</th>\n",
       "      <th>f_5</th>\n",
       "      <th>f_6</th>\n",
       "      <th>f_7</th>\n",
       "      <th>f_8</th>\n",
       "      <th>f_9</th>\n",
       "      <th>...</th>\n",
       "      <th>f_92</th>\n",
       "      <th>f_93</th>\n",
       "      <th>f_94</th>\n",
       "      <th>f_95</th>\n",
       "      <th>f_96</th>\n",
       "      <th>f_97</th>\n",
       "      <th>f_98</th>\n",
       "      <th>f_99</th>\n",
       "      <th>target</th>\n",
       "      <th>kfold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.774555</td>\n",
       "      <td>-1.773382</td>\n",
       "      <td>1.130778</td>\n",
       "      <td>0.815117</td>\n",
       "      <td>-1.594849</td>\n",
       "      <td>-0.032617</td>\n",
       "      <td>-0.154090</td>\n",
       "      <td>0.323289</td>\n",
       "      <td>1.472164</td>\n",
       "      <td>-0.927411</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.086707</td>\n",
       "      <td>0.711119</td>\n",
       "      <td>-1.311827</td>\n",
       "      <td>-0.774696</td>\n",
       "      <td>0.786107</td>\n",
       "      <td>1.254459</td>\n",
       "      <td>1.248586</td>\n",
       "      <td>0.010472</td>\n",
       "      <td>-87.727574</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.612155</td>\n",
       "      <td>0.055304</td>\n",
       "      <td>-0.641272</td>\n",
       "      <td>0.742241</td>\n",
       "      <td>0.776100</td>\n",
       "      <td>0.318328</td>\n",
       "      <td>0.899198</td>\n",
       "      <td>1.491930</td>\n",
       "      <td>1.129125</td>\n",
       "      <td>-0.124982</td>\n",
       "      <td>...</td>\n",
       "      <td>1.590786</td>\n",
       "      <td>0.199018</td>\n",
       "      <td>-0.826898</td>\n",
       "      <td>0.696414</td>\n",
       "      <td>0.537090</td>\n",
       "      <td>-0.205029</td>\n",
       "      <td>0.553149</td>\n",
       "      <td>0.352159</td>\n",
       "      <td>62.675920</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.272502</td>\n",
       "      <td>-0.226216</td>\n",
       "      <td>-0.560199</td>\n",
       "      <td>0.201055</td>\n",
       "      <td>0.727764</td>\n",
       "      <td>-0.554591</td>\n",
       "      <td>0.730228</td>\n",
       "      <td>-0.536563</td>\n",
       "      <td>-0.388146</td>\n",
       "      <td>2.473579</td>\n",
       "      <td>...</td>\n",
       "      <td>0.644352</td>\n",
       "      <td>-0.386575</td>\n",
       "      <td>0.486080</td>\n",
       "      <td>0.092672</td>\n",
       "      <td>-0.844465</td>\n",
       "      <td>0.728180</td>\n",
       "      <td>-0.695450</td>\n",
       "      <td>1.475788</td>\n",
       "      <td>350.866704</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.154180</td>\n",
       "      <td>0.747576</td>\n",
       "      <td>1.289414</td>\n",
       "      <td>-1.210075</td>\n",
       "      <td>0.055685</td>\n",
       "      <td>-0.420973</td>\n",
       "      <td>-1.619377</td>\n",
       "      <td>-0.297873</td>\n",
       "      <td>0.604649</td>\n",
       "      <td>0.276335</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.184078</td>\n",
       "      <td>-0.256804</td>\n",
       "      <td>-1.108195</td>\n",
       "      <td>1.189292</td>\n",
       "      <td>-0.070460</td>\n",
       "      <td>0.412885</td>\n",
       "      <td>-0.029267</td>\n",
       "      <td>-2.991813</td>\n",
       "      <td>-137.205989</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.360387</td>\n",
       "      <td>-0.529910</td>\n",
       "      <td>-0.497947</td>\n",
       "      <td>-0.898662</td>\n",
       "      <td>-0.544286</td>\n",
       "      <td>-0.971647</td>\n",
       "      <td>-1.028042</td>\n",
       "      <td>0.014763</td>\n",
       "      <td>0.540153</td>\n",
       "      <td>-1.292305</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.909205</td>\n",
       "      <td>0.568997</td>\n",
       "      <td>0.924149</td>\n",
       "      <td>-0.791840</td>\n",
       "      <td>-0.381154</td>\n",
       "      <td>0.674265</td>\n",
       "      <td>-2.442922</td>\n",
       "      <td>0.264026</td>\n",
       "      <td>86.090442</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14995</th>\n",
       "      <td>0.294899</td>\n",
       "      <td>-0.835134</td>\n",
       "      <td>0.378512</td>\n",
       "      <td>-0.093931</td>\n",
       "      <td>0.068189</td>\n",
       "      <td>0.270619</td>\n",
       "      <td>1.015385</td>\n",
       "      <td>-0.456247</td>\n",
       "      <td>-1.766488</td>\n",
       "      <td>1.166689</td>\n",
       "      <td>...</td>\n",
       "      <td>2.324996</td>\n",
       "      <td>1.593860</td>\n",
       "      <td>-2.335581</td>\n",
       "      <td>0.613897</td>\n",
       "      <td>-1.096858</td>\n",
       "      <td>1.779097</td>\n",
       "      <td>2.686735</td>\n",
       "      <td>0.701123</td>\n",
       "      <td>48.552951</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14996</th>\n",
       "      <td>0.725418</td>\n",
       "      <td>0.045458</td>\n",
       "      <td>1.621616</td>\n",
       "      <td>-1.821775</td>\n",
       "      <td>-0.549676</td>\n",
       "      <td>-0.187827</td>\n",
       "      <td>1.520519</td>\n",
       "      <td>-1.053870</td>\n",
       "      <td>-1.156351</td>\n",
       "      <td>1.425287</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.296561</td>\n",
       "      <td>0.589363</td>\n",
       "      <td>-1.001864</td>\n",
       "      <td>0.970184</td>\n",
       "      <td>0.525301</td>\n",
       "      <td>1.446674</td>\n",
       "      <td>0.078825</td>\n",
       "      <td>0.775349</td>\n",
       "      <td>-209.246668</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14997</th>\n",
       "      <td>0.754218</td>\n",
       "      <td>0.890882</td>\n",
       "      <td>-0.288758</td>\n",
       "      <td>-0.129470</td>\n",
       "      <td>1.565819</td>\n",
       "      <td>-0.031624</td>\n",
       "      <td>-0.922864</td>\n",
       "      <td>0.372079</td>\n",
       "      <td>-0.208797</td>\n",
       "      <td>-0.145711</td>\n",
       "      <td>...</td>\n",
       "      <td>2.358575</td>\n",
       "      <td>-0.079931</td>\n",
       "      <td>2.464107</td>\n",
       "      <td>-0.257328</td>\n",
       "      <td>0.058097</td>\n",
       "      <td>0.879807</td>\n",
       "      <td>-0.152887</td>\n",
       "      <td>0.135439</td>\n",
       "      <td>34.582027</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14998</th>\n",
       "      <td>0.620541</td>\n",
       "      <td>0.571963</td>\n",
       "      <td>1.380499</td>\n",
       "      <td>-0.253306</td>\n",
       "      <td>-0.693747</td>\n",
       "      <td>-1.205632</td>\n",
       "      <td>0.052820</td>\n",
       "      <td>0.041744</td>\n",
       "      <td>0.547760</td>\n",
       "      <td>-0.788660</td>\n",
       "      <td>...</td>\n",
       "      <td>0.260384</td>\n",
       "      <td>-0.726069</td>\n",
       "      <td>-1.537830</td>\n",
       "      <td>1.020104</td>\n",
       "      <td>-0.362191</td>\n",
       "      <td>-1.066223</td>\n",
       "      <td>1.512754</td>\n",
       "      <td>1.195384</td>\n",
       "      <td>89.142345</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14999</th>\n",
       "      <td>0.263252</td>\n",
       "      <td>0.130420</td>\n",
       "      <td>-0.941442</td>\n",
       "      <td>2.055287</td>\n",
       "      <td>1.580660</td>\n",
       "      <td>0.452101</td>\n",
       "      <td>-0.757829</td>\n",
       "      <td>-1.802759</td>\n",
       "      <td>-1.997433</td>\n",
       "      <td>-0.184177</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.283234</td>\n",
       "      <td>-0.499605</td>\n",
       "      <td>1.671307</td>\n",
       "      <td>-0.117934</td>\n",
       "      <td>-2.971949</td>\n",
       "      <td>-1.539975</td>\n",
       "      <td>-0.106894</td>\n",
       "      <td>-0.551639</td>\n",
       "      <td>272.106874</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15000 rows Ã— 102 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            f_0       f_1       f_2       f_3       f_4       f_5       f_6  \\\n",
       "0     -0.774555 -1.773382  1.130778  0.815117 -1.594849 -0.032617 -0.154090   \n",
       "1      0.612155  0.055304 -0.641272  0.742241  0.776100  0.318328  0.899198   \n",
       "2     -0.272502 -0.226216 -0.560199  0.201055  0.727764 -0.554591  0.730228   \n",
       "3      0.154180  0.747576  1.289414 -1.210075  0.055685 -0.420973 -1.619377   \n",
       "4     -0.360387 -0.529910 -0.497947 -0.898662 -0.544286 -0.971647 -1.028042   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "14995  0.294899 -0.835134  0.378512 -0.093931  0.068189  0.270619  1.015385   \n",
       "14996  0.725418  0.045458  1.621616 -1.821775 -0.549676 -0.187827  1.520519   \n",
       "14997  0.754218  0.890882 -0.288758 -0.129470  1.565819 -0.031624 -0.922864   \n",
       "14998  0.620541  0.571963  1.380499 -0.253306 -0.693747 -1.205632  0.052820   \n",
       "14999  0.263252  0.130420 -0.941442  2.055287  1.580660  0.452101 -0.757829   \n",
       "\n",
       "            f_7       f_8       f_9  ...      f_92      f_93      f_94  \\\n",
       "0      0.323289  1.472164 -0.927411  ... -1.086707  0.711119 -1.311827   \n",
       "1      1.491930  1.129125 -0.124982  ...  1.590786  0.199018 -0.826898   \n",
       "2     -0.536563 -0.388146  2.473579  ...  0.644352 -0.386575  0.486080   \n",
       "3     -0.297873  0.604649  0.276335  ... -0.184078 -0.256804 -1.108195   \n",
       "4      0.014763  0.540153 -1.292305  ... -0.909205  0.568997  0.924149   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "14995 -0.456247 -1.766488  1.166689  ...  2.324996  1.593860 -2.335581   \n",
       "14996 -1.053870 -1.156351  1.425287  ... -1.296561  0.589363 -1.001864   \n",
       "14997  0.372079 -0.208797 -0.145711  ...  2.358575 -0.079931  2.464107   \n",
       "14998  0.041744  0.547760 -0.788660  ...  0.260384 -0.726069 -1.537830   \n",
       "14999 -1.802759 -1.997433 -0.184177  ... -0.283234 -0.499605  1.671307   \n",
       "\n",
       "           f_95      f_96      f_97      f_98      f_99      target  kfold  \n",
       "0     -0.774696  0.786107  1.254459  1.248586  0.010472  -87.727574      0  \n",
       "1      0.696414  0.537090 -0.205029  0.553149  0.352159   62.675920      0  \n",
       "2      0.092672 -0.844465  0.728180 -0.695450  1.475788  350.866704      0  \n",
       "3      1.189292 -0.070460  0.412885 -0.029267 -2.991813 -137.205989      0  \n",
       "4     -0.791840 -0.381154  0.674265 -2.442922  0.264026   86.090442      0  \n",
       "...         ...       ...       ...       ...       ...         ...    ...  \n",
       "14995  0.613897 -1.096858  1.779097  2.686735  0.701123   48.552951      4  \n",
       "14996  0.970184  0.525301  1.446674  0.078825  0.775349 -209.246668      4  \n",
       "14997 -0.257328  0.058097  0.879807 -0.152887  0.135439   34.582027      4  \n",
       "14998  1.020104 -0.362191 -1.066223  1.512754  1.195384   89.142345      4  \n",
       "14999 -0.117934 -2.971949 -1.539975 -0.106894 -0.551639  272.106874      4  \n",
       "\n",
       "[15000 rows x 102 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# stratified kfold for regression\n",
    "def create_folds(data):\n",
    "    # create a new column call kfold and fill with -1\n",
    "    data[\"kfold\"] = -1\n",
    "    \n",
    "    # randomize the rows of data\n",
    "    data = data.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    # calculate the number of bins using Sturge's Rule\n",
    "    num_bins = int(np.floor(1 + np.log2(len(data))))\n",
    "    \n",
    "    # bin targets\n",
    "    data.loc[:, \"bins\"] = pd.cut(\n",
    "    data[\"target\"], bins=num_bins, labels=False\n",
    "    )\n",
    "    \n",
    "    # initiate the kfold class from model_selection module\n",
    "    kf = model_selection.StratifiedKFold(n_splits=5)\n",
    "    \n",
    "    # fill the new kfold column\n",
    "    # note instead of targets, bins are used\n",
    "    for f, (t_, v_) in enumerate(kf.split(X=data, y=data.bins.values)):\n",
    "        data.loc[v_, 'kfold'] = f\n",
    "        \n",
    "    # drop the bins column\n",
    "    data = data.drop(\"bins\", axis=1)\n",
    "    # return df with folds\n",
    "    return data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # here we create a sample dataset with 15000 samples and 100 features and 1 target\n",
    "    X, y = datasets.make_regression(\n",
    "        n_samples=15000, n_features=100, n_targets=1\n",
    "    )\n",
    "    \n",
    "    # create a df out of our np arrays\n",
    "    df = pd.DataFrame(\n",
    "        X,\n",
    "        columns=[f\"f_{i}\" for i in range(X.shape[1])]\n",
    "    )\n",
    "    df.loc[:, \"target\"] = y\n",
    "    \n",
    "    # create folds\n",
    "    df = create_folds(df)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3eaa19a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross validation is the first and most important step when building ml models\n",
    "# whatever ml is being done, split the data first\n",
    "# if a good cross validation scheme is used, where validation (test) data is representative of training and real-world data\n",
    "# then you can build a good ml model\n",
    "\n",
    "# the cross validation scheme used depends on the data, and often the scheme may have to be adapted to suit the data\n",
    "# for example, if the problem is to build a model that takes a picture of a patient as identifies if skin cancer is present\n",
    "# then stratified classification would be the go to, as it is a binary classification problem\n",
    "# however, there will be multiple images of the same patient in the training dataset, and therefore you need to make sure\n",
    "# the same patient isn't in both the training and test dataset. Sci-kit learn has a built in called GroupKFold where the \n",
    "# patients would be considered as groups. Currently you cannot combine stratified and group so this needs to be done by \n",
    "# the user"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
